{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformer import Transformer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"german_novels.txt\", \"r\") as f:\n",
    "    \n",
    "    base_text = f.read()\n",
    "\n",
    "with open(\"gedichte.txt\", \"r\") as f:\n",
    "    \n",
    "    train_text = f.read()\n",
    "    \n",
    "with open(\"gedichte_val.txt\", \"r\") as f:\n",
    "    \n",
    "    val_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_base = len(base_text) // 9\n",
    "\n",
    "# 36 mb is way so much, use 9mb instead\n",
    "base_train_text = base_text[:end_base]\n",
    "base_val_text = base_text[end_base: end_base + end_base // 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sorted(set(base_train_text + base_val_text + train_text + val_text)))\n",
    "\n",
    "c_to_i = {c : i for i, c in enumerate(chars)}\n",
    "i_to_c = {i : c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(s):\n",
    "    return list(map(lambda c: c_to_i[c], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_tokens(t):\n",
    "    return \"\".join(list(map(lambda i: i_to_c[i], t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_tokens = to_tokens(base_train_text)\n",
    "base_val_tokens = to_tokens(base_train_text)\n",
    "\n",
    "train_tokens = to_tokens(train_text)\n",
    "val_tokens = to_tokens(val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, token_list, seq_length):\n",
    "        self.token_list = token_list\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_list) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.token_list[idx:idx+self.seq_length]),\n",
    "                torch.tensor(self.token_list[idx+1:idx+self.seq_length+1]))\n",
    "\n",
    "def get_XY(token_list, seq_length):\n",
    "    dataset = TokenDataset(token_list, seq_length)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 128\n",
    "\n",
    "base_train_dataset = get_XY(base_train_tokens, seq_length)\n",
    "base_val_dataset = get_XY(base_val_tokens, seq_length)\n",
    "train_dataset = get_XY(train_tokens, seq_length)\n",
    "val_dataset = get_XY(val_tokens, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 256\n",
    "vocab_size = len(chars)\n",
    "n_attention_units = 3\n",
    "\n",
    "tf = Transformer(vocab_size = vocab_size, seq_length = seq_length, emb_size = emb_size\\\n",
    "                 , n_attention_units = n_attention_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 512\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optim = torch.optim.Adam(tf.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_dataloader = DataLoader(base_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "base_val_dataloader = DataLoader(base_val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(tf, train_dataloader, test_dataloader, n_epochs):\n",
    "    train_loss_plotting = []\n",
    "    val_loss_plotting = []\n",
    "\n",
    "    for epoch_idx in range(n_epochs):\n",
    "\n",
    "        losses_batches_in_epoch = []\n",
    "\n",
    "        # important for dropout (gets activated)\n",
    "        tf.train()\n",
    "\n",
    "        for batch_X, batch_y in tqdm(train_dataloader):\n",
    "\n",
    "            # send to GPU\n",
    "            batch_X = batch_X.to(tf.device)\n",
    "            batch_y = batch_y.to(tf.device)\n",
    "\n",
    "            logits = tf(batch_X)\n",
    "\n",
    "            # nn.CrossEntropy needs [batch size, n_chars, seq_length]\n",
    "\n",
    "            logits = logits.transpose(1, 2)\n",
    "\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            losses_batches_in_epoch.append(loss.item())\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        losses_batches_in_val = []\n",
    "\n",
    "        # dropout gets deactivated\n",
    "        tf.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch_X, batch_y in val_dataloader:\n",
    "\n",
    "                batch_X = batch_X.to(tf.device)\n",
    "                batch_y = batch_y.to(tf.device)\n",
    "\n",
    "                logits = tf(batch_X)\n",
    "\n",
    "                logits = logits.transpose(1, 2)\n",
    "\n",
    "                loss = criterion(logits, batch_y)\n",
    "\n",
    "                losses_batches_in_val.append(loss.item())\n",
    "\n",
    "        train_loss_epoch = np.mean(np.array(losses_batches_in_epoch))\n",
    "        val_loss_epoch = np.mean(np.array(losses_batches_in_val))\n",
    "\n",
    "        train_loss_plotting.append(train_loss_epoch)\n",
    "        val_loss_plotting.append(val_loss_epoch)\n",
    "    \n",
    "        print(f\"{epoch_idx + 1}. train loss = {train_loss_epoch}, val loss = {val_loss_epoch}\")\n",
    "    \n",
    "    return train_loss_plotting, val_loss_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1269/1269 [03:40<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. train loss = 1.0988311027334468, val loss = 1.3883833562102272\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss_plotting' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train the \"foundation model to learn german\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 60\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(tf, train_dataloader, test_dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     val_loss_plotting\u001b[38;5;241m.\u001b[39mappend(val_loss_epoch)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. train loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss_plotting, \u001b[43mtest_loss_plotting\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss_plotting' is not defined"
     ]
    }
   ],
   "source": [
    "# train the \"foundation model to learn german\"\n",
    "train(tf, train_dataloader, val_dataloader, n_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_transformer(tf, prompt, temperature, n_samples, seq_length):\n",
    "    \n",
    "    tf.eval()\n",
    "    \n",
    "    curr_context = torch.tensor(to_tokens(prompt)).unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    generated = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for _ in range(n_samples):\n",
    "\n",
    "            logits = tf(curr_context)\n",
    "\n",
    "            logits_last = logits[0, -1] / temperature\n",
    "\n",
    "            sampled = torch.distributions.Categorical(logits = logits_last).sample()\n",
    "\n",
    "            generated.append(sampled.item())\n",
    "            \n",
    "            if len(curr_context[0]) < seq_length:\n",
    "                \n",
    "                # add to context\n",
    "                \n",
    "                # unsqueeze(0) to make a batch\n",
    "                # unsqueeze(0) to convert a number into a singleton list\n",
    "                curr_context = torch.cat((curr_context, sampled.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "            else:\n",
    "                \n",
    "                curr_context = torch.cat((curr_context[:, 1:], sampled.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "                \n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tf.state_dict(), \"tuned_model_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = sample_from_transformer(tf, prompt=\"Gibt es Gott?\\n\", temperature=0.8, \\\n",
    "                              n_samples = 1000, seq_length = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Vor ihr Spruch und Wald und um die Geliebte\n",
      "    Unberühren her.\n",
      "    Auf deiner Augen und dazwischen Felsenschlicht\n",
      "    Und gingen niederschwerer,\n",
      "    Mit einer alten Kopf tief verrollt\n",
      "    In einem Bilde des Marmors Bruder,\n",
      "    Schwarm erlösen wohl über seine Rose allee\n",
      "           Wie alles, warum deine frühe Sterne\n",
      "                                         Aber durchbricht mit Quardianeen,\n",
      "           die Gedanken, Bächlein, weiche ich's, wie es wogend durch die Schwalbe steht:\n",
      "    Gewaltig war dieses Rosenlicht.\n",
      "    Den Raum des Herzens war der Jahre schwoll,\n",
      "    Ein Jugend betet sie mit allem die Nacht,\n",
      "    Und da ging schon der Flur!\n",
      "    Nicht mehr will ich erkennen.\n",
      "\n",
      "    Ich will mich beschwingen\n",
      "    Von der süßen Wegen\n",
      "    Und halb im Hafenfedern herunter\n",
      "    In den Hals und der Nebel führten\n",
      "    Wie deine Hand das müttert.\n",
      "\n",
      "    Und immer fragt die Nächte heben\n",
      "    Was schloß ihr düsterrote Rosen!\n",
      "\n",
      "    Nie selten schliefen sich.\n",
      "\n",
      "    In das Sterne kühlt?\n",
      "    Bei Stand meines K\n"
     ]
    }
   ],
   "source": [
    "print(from_tokens(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inpainting",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
